{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgIENEU9_SV1"
      },
      "source": [
        "# Multiclass semantic segmentation using DeepLabV3+\n",
        "\n",
        "**Author:** [Soumik Rakshit](http://github.com/soumik12345)<br>\n",
        "**Date created:** 2021/08/31<br>\n",
        "**Last modified:** 2023/01/06<br>\n",
        "**Description:** Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud-ZwAUY_SV7"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Semantic segmentation, with the goal to assign semantic labels to every pixel in an image,\n",
        "is an essential computer vision task. In this example, we implement\n",
        "the **DeepLabV3+** model for multi-class semantic segmentation, a fully-convolutional\n",
        "architecture that performs well on semantic segmentation benchmarks.\n",
        "\n",
        "### References:\n",
        "\n",
        "- [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n",
        "- [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)\n",
        "- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s529ge6_SV9"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We will use the [Crowd Instance-level Human Parsing Dataset](https://arxiv.org/abs/1811.12596)\n",
        "for training our model. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images.\n",
        "Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification.\n",
        "This dataset can be used for the \"human part segmentation\" task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZiSCOVJH_SV-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VlTzyIqj_SWA",
        "outputId": "999d5257-910f-4f05-a0b6-e74ddeb3e018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&confirm=t\n",
            "To: /content/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET/instance-level-human-parsing.zip\n",
            "100% 2.91G/2.91G [00:34<00:00, 83.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&confirm=t\"\n",
        "!unzip -q instance-level-human-parsing.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nikhilroxtomar/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET.git"
      ],
      "metadata": {
        "id": "tDEOZBw7AWfX",
        "outputId": "345e87ca-f8fb-434a-98f8-ce7aa03094b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 82 (delta 2), reused 0 (delta 0), pack-reused 73\u001b[K\n",
            "Receiving objects: 100% (82/82), 16.70 MiB | 8.15 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "vxbxYshUAZgM",
        "outputId": "460ae3ee-405f-4db8-f3ea-210d02a9d704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET"
      ],
      "metadata": {
        "id": "CTIlBTU_AWhc",
        "outputId": "738ae290-eb14-4050-fcc4-90d1b58cd71f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Multiclass-Segmentation-on-Crowd-Instance-level-Human-Parsing-CHIP-Dataset-using-UNET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSCU3X7cBQGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "O0Q1WPrxAnFh",
        "outputId": "19ed2374-5ecf-4f15-d091-a00a42873c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 120/120 - Valid: 40/40 - Test: 40/40\n",
            "\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 3.0416\n",
            "Epoch 1: val_loss improved from inf to 2.89709, saving model to files/model.h5\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "8/8 [==============================] - 109s 6s/step - loss: 3.0416 - val_loss: 2.8971 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.6968\n",
            "Epoch 2: val_loss improved from 2.89709 to 2.87688, saving model to files/model.h5\n",
            "8/8 [==============================] - 21s 3s/step - loss: 2.6968 - val_loss: 2.8769 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.4610\n",
            "Epoch 3: val_loss improved from 2.87688 to 2.85287, saving model to files/model.h5\n",
            "8/8 [==============================] - 20s 2s/step - loss: 2.4610 - val_loss: 2.8529 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.2972\n",
            "Epoch 4: val_loss improved from 2.85287 to 2.80321, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 2.2972 - val_loss: 2.8032 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.1634\n",
            "Epoch 5: val_loss improved from 2.80321 to 2.73586, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 2.1634 - val_loss: 2.7359 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.0595\n",
            "Epoch 6: val_loss improved from 2.73586 to 2.68310, saving model to files/model.h5\n",
            "8/8 [==============================] - 20s 3s/step - loss: 2.0595 - val_loss: 2.6831 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.9578\n",
            "Epoch 7: val_loss improved from 2.68310 to 2.61480, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.9578 - val_loss: 2.6148 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.8906\n",
            "Epoch 8: val_loss improved from 2.61480 to 2.58310, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.8906 - val_loss: 2.5831 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.8592\n",
            "Epoch 9: val_loss did not improve from 2.58310\n",
            "8/8 [==============================] - 19s 2s/step - loss: 1.8592 - val_loss: 2.5888 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.8205\n",
            "Epoch 10: val_loss improved from 2.58310 to 2.49234, saving model to files/model.h5\n",
            "8/8 [==============================] - 60s 8s/step - loss: 1.8205 - val_loss: 2.4923 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.7529\n",
            "Epoch 11: val_loss improved from 2.49234 to 2.37864, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.7529 - val_loss: 2.3786 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.7113\n",
            "Epoch 12: val_loss improved from 2.37864 to 2.34230, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 1.7113 - val_loss: 2.3423 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.6847\n",
            "Epoch 13: val_loss improved from 2.34230 to 2.19878, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.6847 - val_loss: 2.1988 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.6254\n",
            "Epoch 14: val_loss improved from 2.19878 to 2.15396, saving model to files/model.h5\n",
            "8/8 [==============================] - 20s 2s/step - loss: 1.6254 - val_loss: 2.1540 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.6205\n",
            "Epoch 15: val_loss did not improve from 2.15396\n",
            "8/8 [==============================] - 21s 3s/step - loss: 1.6205 - val_loss: 2.1623 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.5496\n",
            "Epoch 16: val_loss improved from 2.15396 to 2.05665, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 4s/step - loss: 1.5496 - val_loss: 2.0567 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.5458\n",
            "Epoch 17: val_loss improved from 2.05665 to 2.05298, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.5458 - val_loss: 2.0530 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.4784\n",
            "Epoch 18: val_loss improved from 2.05298 to 2.00777, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.4784 - val_loss: 2.0078 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.4545\n",
            "Epoch 19: val_loss improved from 2.00777 to 1.96893, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.4545 - val_loss: 1.9689 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.4299\n",
            "Epoch 20: val_loss improved from 1.96893 to 1.96675, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.4299 - val_loss: 1.9668 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.4068\n",
            "Epoch 21: val_loss improved from 1.96675 to 1.93801, saving model to files/model.h5\n",
            "8/8 [==============================] - 22s 3s/step - loss: 1.4068 - val_loss: 1.9380 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.3910\n",
            "Epoch 22: val_loss improved from 1.93801 to 1.93339, saving model to files/model.h5\n",
            "8/8 [==============================] - 24s 3s/step - loss: 1.3910 - val_loss: 1.9334 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.3642\n",
            "Epoch 23: val_loss improved from 1.93339 to 1.91327, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.3642 - val_loss: 1.9133 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.3476\n",
            "Epoch 24: val_loss improved from 1.91327 to 1.89967, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.3476 - val_loss: 1.8997 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.2970\n",
            "Epoch 25: val_loss improved from 1.89967 to 1.87827, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 1.2970 - val_loss: 1.8783 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.3237\n",
            "Epoch 26: val_loss improved from 1.87827 to 1.86603, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 1.3237 - val_loss: 1.8660 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.2513\n",
            "Epoch 27: val_loss improved from 1.86603 to 1.86508, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.2513 - val_loss: 1.8651 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.2908\n",
            "Epoch 28: val_loss improved from 1.86508 to 1.86192, saving model to files/model.h5\n",
            "8/8 [==============================] - 22s 3s/step - loss: 1.2908 - val_loss: 1.8619 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.2456\n",
            "Epoch 29: val_loss improved from 1.86192 to 1.85104, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 1.2456 - val_loss: 1.8510 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1911\n",
            "Epoch 30: val_loss improved from 1.85104 to 1.84207, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 1.1911 - val_loss: 1.8421 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1690\n",
            "Epoch 31: val_loss did not improve from 1.84207\n",
            "8/8 [==============================] - 21s 2s/step - loss: 1.1690 - val_loss: 1.8457 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1616\n",
            "Epoch 32: val_loss improved from 1.84207 to 1.80533, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.1616 - val_loss: 1.8053 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1344\n",
            "Epoch 33: val_loss improved from 1.80533 to 1.80380, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.1344 - val_loss: 1.8038 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0855\n",
            "Epoch 34: val_loss did not improve from 1.80380\n",
            "8/8 [==============================] - 20s 2s/step - loss: 1.0855 - val_loss: 1.8108 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0917\n",
            "Epoch 35: val_loss improved from 1.80380 to 1.79517, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.0917 - val_loss: 1.7952 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0520\n",
            "Epoch 36: val_loss did not improve from 1.79517\n",
            "8/8 [==============================] - 19s 2s/step - loss: 1.0520 - val_loss: 1.7952 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0626\n",
            "Epoch 37: val_loss did not improve from 1.79517\n",
            "8/8 [==============================] - 19s 2s/step - loss: 1.0626 - val_loss: 1.8080 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0183\n",
            "Epoch 38: val_loss improved from 1.79517 to 1.79458, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 1.0183 - val_loss: 1.7946 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0379\n",
            "Epoch 39: val_loss did not improve from 1.79458\n",
            "8/8 [==============================] - 19s 2s/step - loss: 1.0379 - val_loss: 1.8082 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9932\n",
            "Epoch 40: val_loss did not improve from 1.79458\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.9932 - val_loss: 1.7976 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9710\n",
            "Epoch 41: val_loss did not improve from 1.79458\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.9710 - val_loss: 1.8327 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9453\n",
            "Epoch 42: val_loss did not improve from 1.79458\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.9453 - val_loss: 1.8090 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9298\n",
            "Epoch 43: val_loss improved from 1.79458 to 1.77262, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.9298 - val_loss: 1.7726 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9342\n",
            "Epoch 44: val_loss improved from 1.77262 to 1.77012, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.9342 - val_loss: 1.7701 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9197\n",
            "Epoch 45: val_loss did not improve from 1.77012\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.9197 - val_loss: 1.7740 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9027\n",
            "Epoch 46: val_loss improved from 1.77012 to 1.76691, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.9027 - val_loss: 1.7669 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8626\n",
            "Epoch 47: val_loss improved from 1.76691 to 1.76406, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.8626 - val_loss: 1.7641 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8732\n",
            "Epoch 48: val_loss improved from 1.76406 to 1.73530, saving model to files/model.h5\n",
            "8/8 [==============================] - 32s 4s/step - loss: 0.8732 - val_loss: 1.7353 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8243\n",
            "Epoch 49: val_loss improved from 1.73530 to 1.72919, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.8243 - val_loss: 1.7292 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8017\n",
            "Epoch 50: val_loss improved from 1.72919 to 1.71974, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.8017 - val_loss: 1.7197 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7822\n",
            "Epoch 51: val_loss improved from 1.71974 to 1.70532, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 0.7822 - val_loss: 1.7053 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7518\n",
            "Epoch 52: val_loss did not improve from 1.70532\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.7518 - val_loss: 1.7284 - lr: 1.0000e-04\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7715\n",
            "Epoch 53: val_loss improved from 1.70532 to 1.69701, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.7715 - val_loss: 1.6970 - lr: 1.0000e-04\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7693\n",
            "Epoch 54: val_loss did not improve from 1.69701\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7693 - val_loss: 1.7398 - lr: 1.0000e-04\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7595\n",
            "Epoch 55: val_loss improved from 1.69701 to 1.63307, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.7595 - val_loss: 1.6331 - lr: 1.0000e-04\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7574\n",
            "Epoch 56: val_loss improved from 1.63307 to 1.62861, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 0.7574 - val_loss: 1.6286 - lr: 1.0000e-04\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7490\n",
            "Epoch 57: val_loss did not improve from 1.62861\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.7490 - val_loss: 1.6488 - lr: 1.0000e-04\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7072\n",
            "Epoch 58: val_loss did not improve from 1.62861\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7072 - val_loss: 1.6312 - lr: 1.0000e-04\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7036\n",
            "Epoch 59: val_loss improved from 1.62861 to 1.59581, saving model to files/model.h5\n",
            "8/8 [==============================] - 28s 4s/step - loss: 0.7036 - val_loss: 1.5958 - lr: 1.0000e-04\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6655\n",
            "Epoch 60: val_loss did not improve from 1.59581\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.6655 - val_loss: 1.6154 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6726\n",
            "Epoch 61: val_loss did not improve from 1.59581\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.6726 - val_loss: 1.6089 - lr: 1.0000e-04\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6468\n",
            "Epoch 62: val_loss improved from 1.59581 to 1.58757, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.6468 - val_loss: 1.5876 - lr: 1.0000e-04\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6320\n",
            "Epoch 63: val_loss improved from 1.58757 to 1.56569, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.6320 - val_loss: 1.5657 - lr: 1.0000e-04\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6402\n",
            "Epoch 64: val_loss did not improve from 1.56569\n",
            "8/8 [==============================] - 21s 3s/step - loss: 0.6402 - val_loss: 1.5918 - lr: 1.0000e-04\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5834\n",
            "Epoch 65: val_loss improved from 1.56569 to 1.54036, saving model to files/model.h5\n",
            "8/8 [==============================] - 27s 3s/step - loss: 0.5834 - val_loss: 1.5404 - lr: 1.0000e-04\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5724\n",
            "Epoch 66: val_loss did not improve from 1.54036\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.5724 - val_loss: 1.6071 - lr: 1.0000e-04\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5435\n",
            "Epoch 67: val_loss did not improve from 1.54036\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.5435 - val_loss: 1.6010 - lr: 1.0000e-04\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5578\n",
            "Epoch 68: val_loss did not improve from 1.54036\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.5578 - val_loss: 1.5809 - lr: 1.0000e-04\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5502\n",
            "Epoch 69: val_loss did not improve from 1.54036\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.5502 - val_loss: 1.5782 - lr: 1.0000e-04\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5519\n",
            "Epoch 70: val_loss improved from 1.54036 to 1.52191, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.5519 - val_loss: 1.5219 - lr: 1.0000e-04\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5250\n",
            "Epoch 71: val_loss did not improve from 1.52191\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.5250 - val_loss: 1.6137 - lr: 1.0000e-04\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5221\n",
            "Epoch 72: val_loss did not improve from 1.52191\n",
            "8/8 [==============================] - 21s 3s/step - loss: 0.5221 - val_loss: 1.6298 - lr: 1.0000e-04\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4751\n",
            "Epoch 73: val_loss did not improve from 1.52191\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.4751 - val_loss: 1.6131 - lr: 1.0000e-04\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4498\n",
            "Epoch 74: val_loss did not improve from 1.52191\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.4498 - val_loss: 1.5451 - lr: 1.0000e-04\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4417\n",
            "Epoch 75: val_loss did not improve from 1.52191\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.4417 - val_loss: 1.5478 - lr: 1.0000e-04\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4087\n",
            "Epoch 76: val_loss improved from 1.52191 to 1.51214, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.4087 - val_loss: 1.5121 - lr: 1.0000e-05\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4083\n",
            "Epoch 77: val_loss improved from 1.51214 to 1.48694, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.4083 - val_loss: 1.4869 - lr: 1.0000e-05\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3957\n",
            "Epoch 78: val_loss did not improve from 1.48694\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.3957 - val_loss: 1.4892 - lr: 1.0000e-05\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3841\n",
            "Epoch 79: val_loss improved from 1.48694 to 1.48221, saving model to files/model.h5\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.3841 - val_loss: 1.4822 - lr: 1.0000e-05\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3748\n",
            "Epoch 80: val_loss improved from 1.48221 to 1.47617, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.3748 - val_loss: 1.4762 - lr: 1.0000e-05\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3754\n",
            "Epoch 81: val_loss improved from 1.47617 to 1.47020, saving model to files/model.h5\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3754 - val_loss: 1.4702 - lr: 1.0000e-05\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3600\n",
            "Epoch 82: val_loss improved from 1.47020 to 1.46374, saving model to files/model.h5\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3600 - val_loss: 1.4637 - lr: 1.0000e-05\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3606\n",
            "Epoch 83: val_loss improved from 1.46374 to 1.44796, saving model to files/model.h5\n",
            "8/8 [==============================] - 25s 3s/step - loss: 0.3606 - val_loss: 1.4480 - lr: 1.0000e-05\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3765\n",
            "Epoch 84: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3765 - val_loss: 1.4482 - lr: 1.0000e-05\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3665\n",
            "Epoch 85: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3665 - val_loss: 1.4612 - lr: 1.0000e-05\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3478\n",
            "Epoch 86: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.3478 - val_loss: 1.4554 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3643\n",
            "Epoch 87: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3643 - val_loss: 1.4509 - lr: 1.0000e-05\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3589\n",
            "Epoch 88: val_loss did not improve from 1.44796\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3589 - val_loss: 1.4520 - lr: 1.0000e-05\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3413\n",
            "Epoch 89: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3413 - val_loss: 1.4534 - lr: 1.0000e-06\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3642\n",
            "Epoch 90: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3642 - val_loss: 1.4549 - lr: 1.0000e-06\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3509\n",
            "Epoch 91: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3509 - val_loss: 1.4560 - lr: 1.0000e-06\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3666\n",
            "Epoch 92: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3666 - val_loss: 1.4581 - lr: 1.0000e-06\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3575\n",
            "Epoch 93: val_loss did not improve from 1.44796\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3575 - val_loss: 1.4608 - lr: 1.0000e-06\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3403\n",
            "Epoch 94: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.3403 - val_loss: 1.4637 - lr: 1.0000e-07\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3570\n",
            "Epoch 95: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3570 - val_loss: 1.4670 - lr: 1.0000e-07\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3517\n",
            "Epoch 96: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3517 - val_loss: 1.4707 - lr: 1.0000e-07\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3501\n",
            "Epoch 97: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.3501 - val_loss: 1.4742 - lr: 1.0000e-07\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3502\n",
            "Epoch 98: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3502 - val_loss: 1.4766 - lr: 1.0000e-07\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3524\n",
            "Epoch 99: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.3524 - val_loss: 1.4808 - lr: 1.0000e-07\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3529\n",
            "Epoch 100: val_loss did not improve from 1.44796\n",
            "8/8 [==============================] - 26s 3s/step - loss: 0.3529 - val_loss: 1.4835 - lr: 1.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py"
      ],
      "metadata": {
        "id": "_sbIvi9FApqL",
        "outputId": "4007e741-1782-4fda-a8e6-899fcc24e9d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 120/120 - Valid: 40/40 - Test: 40/40\n",
            "\n",
            "100% 40/40 [00:23<00:00,  1.71it/s]\n",
            "Class           F1         Jaccard   \n",
            "-----------------------------------\n",
            "Background     : 0.81099 - 0.69642\n",
            "Hat            : 0.00005 - 0.00002\n",
            "Hair           : 0.31840 - 0.22514\n",
            "Glove          : 0.00000 - 0.00000\n",
            "Sunglasses     : 0.00000 - 0.00000\n",
            "UpperClothes   : 0.23393 - 0.14845\n",
            "Dress          : 0.00065 - 0.00033\n",
            "Coat           : 0.13232 - 0.09571\n",
            "Socks          : 0.00000 - 0.00000\n",
            "Pants          : 0.13721 - 0.09141\n",
            "Torso-skin     : 0.10310 - 0.05975\n",
            "Scarf          : 0.00000 - 0.00000\n",
            "Skirt          : 0.00000 - 0.00000\n",
            "Face           : 0.39730 - 0.28873\n",
            "Left-arm       : 0.12805 - 0.07712\n",
            "Right-arm      : 0.11353 - 0.06501\n",
            "Left-leg       : 0.00000 - 0.00000\n",
            "Right-leg      : 0.00000 - 0.00000\n",
            "Left-shoe      : 0.00000 - 0.00000\n",
            "Right-shoe     : 0.00000 - 0.00000\n",
            "-----------------------------------\n",
            "Mean           : 0.11878 - 0.08741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zwjkd78AWi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51FHIZY6AWkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iznrYKuCAWmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2iQqT4sAWof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LflIZmb6AWqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWMV3WR_SWB"
      },
      "source": [
        "## Creating a TensorFlow Dataset\n",
        "\n",
        "Training on the entire CIHP dataset with 38,280 images takes a lot of time, hence we will be using\n",
        "a smaller subset of 200 images for training our model in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IhdcyzVL_SWB",
        "outputId": "af7ba4a8-2c27-48ca-cc20-6200870c4170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 512, 512, 1), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(4, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 512, 512, 1), dtype=tf.float32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 512\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 20\n",
        "DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n",
        "NUM_TRAIN_IMAGES = 1000\n",
        "NUM_VAL_IMAGES = 50\n",
        "\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "        image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pwv3rAf_SWC"
      },
      "source": [
        "## Building the DeepLabV3+ model\n",
        "\n",
        "DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. The encoder module\n",
        "processes multiscale contextual information by applying dilated convolution at multiple\n",
        "scales, while the decoder module refines the segmentation results along object boundaries.\n",
        "\n",
        "![](https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png)\n",
        "\n",
        "**Dilated convolution:** With dilated convolution, as we go deeper in the network, we can keep the\n",
        "stride constant but with larger field-of-view without increasing the number of parameters\n",
        "or the amount of computation. Besides, it enables larger output feature maps, which is\n",
        "useful for semantic segmentation.\n",
        "\n",
        "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the\n",
        "sampling rate becomes larger, the number of valid filter weights (i.e., weights that\n",
        "are applied to the valid feature region, instead of padded zeros) becomes smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iG7NAINN_SWD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhsT0-w3_SWE"
      },
      "source": [
        "The encoder features are first bilinearly upsampled by a factor 4, and then\n",
        "concatenated with the corresponding low-level features from the network backbone that\n",
        "have the same spatial resolution. For this example, we\n",
        "use a ResNet50 pretrained on ImageNet as the backbone model, and we use\n",
        "the low-level features from the `conv4_block6_2_relu` block of the backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AHGcLA87_SWE",
        "outputId": "c5b0317c-e12f-4ffe-b634-fc9dcb42e1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 512, 512, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 518, 518, 3)          0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 256, 256, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 256, 256, 64)         256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 256, 256, 64)         0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 258, 258, 64)         0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 128, 128, 64)         0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 128, 128, 64)         4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 128, 128, 64)         36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 128, 128, 256)        16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 128, 128, 256)        16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 128, 128, 256)        1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 128, 128, 256)        1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 128, 128, 256)        0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 128, 128, 256)        0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 128, 128, 64)         16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 128, 128, 64)         36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 128, 128, 256)        16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 128, 128, 256)        1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 128, 128, 256)        0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 128, 128, 256)        0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 128, 128, 64)         16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 128, 128, 64)         36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 128, 128, 64)         256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 128, 128, 64)         0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 128, 128, 256)        16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 128, 128, 256)        1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 128, 128, 256)        0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 128, 128, 256)        0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 64, 64, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 64, 64, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 64, 64, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 64, 64, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 64, 64, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 64, 64, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 64, 64, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 64, 64, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 64, 64, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 64, 64, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 64, 64, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 64, 64, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 64, 64, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 64, 64, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 64, 64, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 64, 64, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 64, 64, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 64, 64, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 64, 64, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 64, 64, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 64, 64, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 64, 64, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 64, 64, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 64, 64, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 64, 64, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 64, 64, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 64, 64, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 64, 64, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 32, 32, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 32, 32, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 32, 32, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 32, 32, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 32, 32, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 32, 32, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 32, 32, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 32, 32, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 32, 32, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 32, 32, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 32, 32, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 32, 32, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 32, 32, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 32, 32, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 32, 32, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 32, 32, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 32, 32, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 32, 32, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 32, 32, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 32, 32, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 32, 32, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 32, 32, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 32, 32, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 32, 32, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 32, 32, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 32, 32, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling2d (Average  (None, 1, 1, 256)            0         ['conv4_block6_2_relu[0][0]'] \n",
            " Pooling2D)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 1, 1, 256)            65792     ['average_pooling2d[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1, 1, 256)            1024      ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 256)          65536     ['conv4_block6_2_relu[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 256)          589824    ['conv4_block6_2_relu[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 256)          589824    ['conv4_block6_2_relu[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 256)          589824    ['conv4_block6_2_relu[0][0]'] \n",
            "                                                                                                  \n",
            " tf.nn.relu (TFOpLambda)     (None, 1, 1, 256)            0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 256)          1024      ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 32, 32, 256)          1024      ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 32, 32, 256)          1024      ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 32, 32, 256)          1024      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2  (None, 32, 32, 256)          0         ['tf.nn.relu[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 32, 32, 256)          0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 32, 32, 256)          0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)   (None, 32, 32, 256)          0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_4 (TFOpLambda)   (None, 32, 32, 256)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 32, 32, 1280)         0         ['up_sampling2d[0][0]',       \n",
            "                                                                     'tf.nn.relu_1[0][0]',        \n",
            "                                                                     'tf.nn.relu_2[0][0]',        \n",
            "                                                                     'tf.nn.relu_3[0][0]',        \n",
            "                                                                     'tf.nn.relu_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 256)          327680    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 32, 32, 256)          1024      ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 128, 128, 48)         3072      ['conv2_block3_2_relu[0][0]'] \n",
            "                                                                                                  \n",
            " tf.nn.relu_5 (TFOpLambda)   (None, 32, 32, 256)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 128, 128, 48)         192       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSamplin  (None, 128, 128, 256)        0         ['tf.nn.relu_5[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " tf.nn.relu_6 (TFOpLambda)   (None, 128, 128, 48)         0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 128, 128, 304)        0         ['up_sampling2d_1[0][0]',     \n",
            " )                                                                   'tf.nn.relu_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 128, 128, 256)        700416    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 128, 128, 256)        1024      ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_7 (TFOpLambda)   (None, 128, 128, 256)        0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 128, 128, 256)        589824    ['tf.nn.relu_7[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 128, 128, 256)        1024      ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_8 (TFOpLambda)   (None, 128, 128, 256)        0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSamplin  (None, 512, 512, 256)        0         ['tf.nn.relu_8[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 512, 512, 20)         5140      ['up_sampling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11857236 (45.23 MB)\n",
            "Trainable params: 11824500 (45.11 MB)\n",
            "Non-trainable params: 32736 (127.88 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adoVcuXf_SWF"
      },
      "source": [
        "## Training\n",
        "\n",
        "We train the model using sparse categorical crossentropy as the loss function, and\n",
        "Adam as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ntJ1Nw3m_SWF",
        "outputId": "3276d819-0152-4473-c193-de5f089ad0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "116/250 [============>.................] - ETA: 49s - loss: 1.3137 - accuracy: 0.6162"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-177007bb12ea>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY8hZIwd_SWG"
      },
      "source": [
        "## Inference using Colormap Overlay\n",
        "\n",
        "The raw predictions from the model represent a one-hot encoded tensor of shape `(N, 512, 512, 20)`\n",
        "where each one of the 20 channels is a binary mask corresponding to a predicted label.\n",
        "In order to visualize the results, we plot them as RGB segmentation masks where each pixel\n",
        "is represented by a unique color corresponding to the particular label predicted. We can easily\n",
        "find the color corresponding to each label from the `human_colormap.mat` file provided as part\n",
        "of the dataset. We would also plot an overlay of the RGB segmentation mask on the input image as\n",
        "this further helps us to identify the different categories present in the image more intuitively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1zL9u3d_SWG"
      },
      "outputs": [],
      "source": [
        "# Loading the Colormap\n",
        "colormap = loadmat(\n",
        "    \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\"\n",
        ")[\"colormap\"]\n",
        "colormap = colormap * 100\n",
        "colormap = colormap.astype(np.uint8)\n",
        "\n",
        "\n",
        "def infer(model, image_tensor):\n",
        "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
        "    predictions = np.squeeze(predictions)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def decode_segmentation_masks(mask, colormap, n_classes):\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    for l in range(0, n_classes):\n",
        "        idx = mask == l\n",
        "        r[idx] = colormap[l, 0]\n",
        "        g[idx] = colormap[l, 1]\n",
        "        b[idx] = colormap[l, 2]\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def get_overlay(image, colored_mask):\n",
        "    image = tf.keras.utils.array_to_img(image)\n",
        "    image = np.array(image).astype(np.uint8)\n",
        "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
        "    return overlay\n",
        "\n",
        "\n",
        "def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
        "    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    for i in range(len(display_list)):\n",
        "        if display_list[i].shape[-1] == 3:\n",
        "            axes[i].imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "        else:\n",
        "            axes[i].imshow(display_list[i])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_predictions(images_list, colormap, model):\n",
        "    for image_file in images_list:\n",
        "        image_tensor = read_image(image_file)\n",
        "        prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
        "        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n",
        "        overlay = get_overlay(image_tensor, prediction_colormap)\n",
        "        plot_samples_matplotlib(\n",
        "            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ6dr6AZ_SWG"
      },
      "source": [
        "### Inference on Train Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxzVRfFn_SWH"
      },
      "outputs": [],
      "source": [
        "plot_predictions(train_images[:4], colormap, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkCMkhqc_SWH"
      },
      "source": [
        "### Inference on Validation Images\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deeplabv3p-resnet50) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Human-Part-Segmentation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13W9Aqj1_SWH"
      },
      "outputs": [],
      "source": [
        "plot_predictions(val_images[:4], colormap, model=model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deeplabv3_plus",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}